{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Efficient LLM fine-tuning\n",
    "\n",
    "We'll go through an example of parameter efficinet GPU training on a T4 GPU by using Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers -q\n",
    "%pip install bitsandbytes -q\n",
    "%pip install datasets -q\n",
    "%pip install accelerate -q\n",
    "%pip install peft -q\n",
    "%pip install trl -q\n",
    "%pip install einops -q\n",
    "%pip install tensorboard -q\n",
    "\n",
    "%pip install watermark -q # version checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 10 01:31:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 27%   36C    P8    18W / 250W |      6MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:2E:00.0 Off |                  N/A |\n",
      "| 27%   36C    P8    20W / 220W |      6MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 27%   37C    P8     5W / 180W |     11MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:61:00.0 Off |                  N/A |\n",
      "| 27%   38C    P8     1W / 180W |      6MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check the GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "We will load the model by using the `transformers` library from Hugging Face.\n",
    "\n",
    "In order to fit really large models into a single GPU, you can\n",
    "load the model in half precision. Most LLMs are even trained in half precision (float16, bfloat16) and there is almost no performace loss compared with full precision (float32) training.\n",
    "\n",
    "If that is not enough, you can quantize the weights of the model to 8bit or even 4bit. For that we need the `accelerate` and `bitsandbytes` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: base (GB): 1.324785664\n",
      "Model: fp16 (GB): 0.662392832\n",
      "Model: 8bit (GB): 0.359354368\n",
      "Model: 4bit (GB): 0.207835136\n"
     ]
    }
   ],
   "source": [
    "# Illustrate memory usage\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\").to(\"cuda\")\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=\"auto\").to(\"cuda\") # torch_dtype=torch.float16\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True) # 8bit and 4bit models are automatically loaded to GPU\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_4bit=True)\n",
    "\n",
    "def print_model_gpu_usage(model, name):\n",
    "  params_size = sum([param.nelement() * param.element_size()\n",
    "                    for param in model.parameters()])  # in bytes\n",
    "  print(f\"Model: {name} (GB): {params_size/1e9}\")\n",
    "\n",
    "print_model_gpu_usage(model, \"base\")\n",
    "print_model_gpu_usage(model_fp16, \"fp16\")\n",
    "print_model_gpu_usage(model_8bit, \"8bit\")\n",
    "print_model_gpu_usage(model_4bit, \"4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decreased model memory usage from 1.32GB to 0.2GB ~ 15% of original size. But loading in 8bit and 4bit comes with a bit of performance degradation. Keep in mind that we also need to save some GPU memory for training, so we don't want to take the whole memory just by loading the model.\n",
    "\n",
    "If we are interested in only running the inference on short sequences, we do not need that much extra memory. Memory requirements also increase if we will be processing very long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "import gc\n",
    "import torch\n",
    "del model\n",
    "del model_fp16\n",
    "del model_8bit\n",
    "del model_4bit\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 10 01:31:55 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 27%   39C    P2    58W / 250W |    666MiB / 11264MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:2E:00.0 Off |                  N/A |\n",
      "| 27%   36C    P8    20W / 220W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 27%   37C    P8     5W / 180W |     14MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:61:00.0 Off |                  N/A |\n",
      "| 27%   38C    P8     1W / 180W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model for Fine-Tuning\n",
    "Now we will load the model that we will be working with in 8bit. If you are working locally, after downloading the model weights for the first time they will be cached, so following model loadings will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falcon base model for tuning\n",
    "model_name = \"tiiuae/falcon-rw-1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: base (GB): 1.41529088\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", load_in_8bit=True)\n",
    "\n",
    "print_model_gpu_usage(model, \"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.word_embeddings.weight                           torch.bfloat16     True    torch.Size([50304, 2048])\n",
      "transformer.h.0.self_attention.query_key_value.weight        torch.int8         False   torch.Size([6144, 2048])\n",
      "transformer.h.0.self_attention.query_key_value.bias          torch.bfloat16     False   torch.Size([6144])\n",
      "transformer.h.0.self_attention.dense.weight                  torch.int8         False   torch.Size([2048, 2048])\n",
      "transformer.h.0.self_attention.dense.bias                    torch.bfloat16     False   torch.Size([2048])\n",
      "transformer.h.0.mlp.dense_h_to_4h.weight                     torch.int8         False   torch.Size([8192, 2048])\n",
      "transformer.h.0.mlp.dense_h_to_4h.bias                       torch.bfloat16     False   torch.Size([8192])\n",
      "transformer.h.0.mlp.dense_4h_to_h.weight                     torch.int8         False   torch.Size([2048, 8192])\n",
      "transformer.h.0.mlp.dense_4h_to_h.bias                       torch.bfloat16     False   torch.Size([2048])\n",
      "transformer.h.0.input_layernorm.weight                       torch.bfloat16     True    torch.Size([2048])\n",
      "transformer.h.0.input_layernorm.bias                         torch.bfloat16     True    torch.Size([2048])\n",
      "transformer.h.0.post_attention_layernorm.weight              torch.bfloat16     True    torch.Size([2048])\n",
      "transformer.h.0.post_attention_layernorm.bias                torch.bfloat16     True    torch.Size([2048])\n",
      "transformer.h.1.self_attention.query_key_value.weight        torch.int8         False   torch.Size([6144, 2048])\n",
      "transformer.h.1.self_attention.query_key_value.bias          torch.bfloat16     False   torch.Size([6144])\n",
      "transformer.h.1.self_attention.dense.weight                  torch.int8         False   torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Print only first 15 layers\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    print(f\"{name:60s} {str(param.dtype):18s} {str(param.requires_grad):6s}  {param.shape}\")\n",
    "    if i == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model in 8bit changed attention query, key, value and mlp weight layers in transformer block to int8 and locked them.\n",
    "Only embedding, layernorm and classification head layers (not visible here) are kept unlocked and loaded in bfloat16.\n",
    "\n",
    "They are loaded in bfloat16 because we set `torch_dtype=\"auto\"` and in the model configuration file bfloat16 is the type used. You can see the model config file in Hugging Face Hub or write `model.config`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT - Parameter Efficient Fine Tuning\n",
    "LLMs are huge! We can't train the whole model for multiple reasons:\n",
    "\n",
    "*   We probably do not have enough data (overfit issues)\n",
    "*   Not enough hardware to run it in a reasonable amount of time\n",
    "*   Very expensive to rent GPUs\n",
    "\n",
    "Solution is to train only a subset of parameters using techniques like LoRA.\n",
    "\n",
    "The idea is to freeze all layers and inject additional layers that will be trained. These additional layers are two lower rank matrices which means that they have less parameters than the original weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "\n",
    "lora_config = peft.LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=16,           # Rank of update matrices, lower the rank lower number of parameters to train\n",
    "    lora_alpha=32,  # LoRA scaling factor\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",  # Should bias parameter be trained (of the original model); if True this will change the base model biases not just Lora AB matrices\n",
    "    target_modules=[\"query_key_value\"] # Print model parameters name to figure out which modules to target\n",
    ")\n",
    "\n",
    "peft_model = peft.get_peft_model(model, lora_config)  # this changes the model in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.word_embeddings.weight                                          torch.bfloat16     False   torch.Size([50304, 2048])\n",
      "transformer.h.0.self_attention.query_key_value.weight                       torch.int8         False   torch.Size([6144, 2048])\n",
      "transformer.h.0.self_attention.query_key_value.bias                         torch.bfloat16     False   torch.Size([6144])\n",
      "transformer.h.0.self_attention.query_key_value.lora_A.default.weight        torch.float32      True    torch.Size([16, 2048])\n",
      "transformer.h.0.self_attention.query_key_value.lora_B.default.weight        torch.float32      True    torch.Size([6144, 16])\n",
      "transformer.h.0.self_attention.dense.weight                                 torch.int8         False   torch.Size([2048, 2048])\n",
      "transformer.h.0.self_attention.dense.bias                                   torch.bfloat16     False   torch.Size([2048])\n",
      "transformer.h.0.mlp.dense_h_to_4h.weight                                    torch.int8         False   torch.Size([8192, 2048])\n",
      "transformer.h.0.mlp.dense_h_to_4h.bias                                      torch.bfloat16     False   torch.Size([8192])\n",
      "transformer.h.0.mlp.dense_4h_to_h.weight                                    torch.int8         False   torch.Size([2048, 8192])\n",
      "transformer.h.0.mlp.dense_4h_to_h.bias                                      torch.bfloat16     False   torch.Size([2048])\n",
      "transformer.h.0.input_layernorm.weight                                      torch.bfloat16     False   torch.Size([2048])\n",
      "transformer.h.0.input_layernorm.bias                                        torch.bfloat16     False   torch.Size([2048])\n",
      "transformer.h.0.post_attention_layernorm.weight                             torch.bfloat16     False   torch.Size([2048])\n",
      "transformer.h.0.post_attention_layernorm.bias                               torch.bfloat16     False   torch.Size([2048])\n",
      "transformer.h.1.self_attention.query_key_value.weight                       torch.int8         False   torch.Size([6144, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Print only first 15 layers\n",
    "for i, (name, param) in enumerate(model.named_parameters()):  # deliberately using model instead of peft_model\n",
    "    print(f\"{name:75s} {str(param.dtype):18s} {str(param.requires_grad):6s}  {param.shape}\")\n",
    "    if i == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how peft added `lora_A` and `lora_B` layers (in float32) and locked all other layers. When multiplied, matrix B*A correspond to `query_key_value.weight` dimension but overall has fewer parameters.\n",
    "\n",
    "Those are the only layers we will be training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,145,728 || all params: 1,314,770,944 || trainable%: 0.2392605354077554\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Tokenizer converts text into small chunks - tokens. Each token is associated with an ID. This list of token IDs is the actual input to LLMs.\n",
    "\n",
    "```\n",
    "#       Text     --->           Tokens               --->    Input ids\n",
    "\"This is great!\" ---> [\"This\", \"_is\", \"_great\", \"!\"] ---> [52, 345, 124, 13]\n",
    "````\n",
    "\n",
    "Note: It is important to use the same tokenizer that was used during model pretraining. In practice, this means loading the tokenizer with the same name as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base models are often trained without padding tokens\n",
    "assert tokenizer.pad_token_id is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often used in various tutorials as a quick PoC fix\n",
    "# We'll see that this approach is not optimal\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [21968, 3303, 4981, 389], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(\"Large language models are\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling the tokenizer directly, it uses the `__call__` method which returns `input_ids` and `attention_mask` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Large', 'Ġlanguage', 'Ġmodels', 'Ġare']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Large language models are\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the tokens first, you can use `tokenizer.tokenize` method.\n",
    "\n",
    "Or you can convert ids to tokens using `tokenizer.convert_ids_to_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Large', 'Ġlanguage', 'Ġmodels', 'Ġare']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attention_mask` is useful for batch processing. Different texts, when tokenized have different number of tokens, but in order to create a batch we need to have the same number of tokens. We can add some token to the end of sequences to make them have the same number of toknes. This token is called padding token. But when we process the text we want to ignore these artificially added tokens so we use `attention_mask`. It tells the attention mechanism which tokens to include and which to ignore during its calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21968, 26992, 1018, 496, 4981, 389], [40, 716, 50256, 50256, 50256, 50256]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Large lanugage models are\", \"I am\"], padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First sentece has 6 tokens, while the second one has only 2 tokens. So we need to add 4 padding tokens (with the ID = 50256). In this case they are paded from the right. Attention mask for first sentence contains only 1s because all tokens are relevant, while in the second sentece, last 4 tokens are padded so attention mask is 0 for these tokens. This way attention mechanism will know to look only first 2 tokens as the relevant ones for this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decode input ids back to text, we can use the `tokenizer.decode` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models are'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_text['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token:  <|endoftext|>\n",
      "Padding token id:  50256\n"
     ]
    }
   ],
   "source": [
    "print(\"Padding token: \", tokenizer.pad_token)\n",
    "print(\"Padding token id: \", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "In order to run inference, we need to:\n",
    "1. Tokenize the text in order to get `input_ids` and `attention_mask`\n",
    "2. Pass `input_ids` and `attention_mask` to `model.generate` function\n",
    "3. Decode the output of the model back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[2061,  389, 1588, 3303, 4981,   30]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "text = \"What are large language models?\"\n",
    "\n",
    "# 1. Tokenize\n",
    "model_input = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")  # return input_ids and attention_mask as pytorch tensors (\"pt\")\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mladen/.pyenv/versions/gpu_efficient_llm_tuning/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2061,   389,  1588,  3303,  4981,    30,   198, 21968,  3303,  4981,\n",
      "           357,  3069,    44,     8,   389,  3303,  4981,   326,   389,  4457,\n",
      "          1588,   287,  1111,   262,  1271,   286, 10007,   290,   287,   262,\n",
      "          2546,   286,   511,  3047,  1366,    13,  1119,   389,   517,  2408,\n",
      "           284,  4512,    11,   475,   389,  6007,   286,  9489,  8861,   326,\n",
      "          2421,  1588,  3146,   286, 10007,   393,  1588,  6867,   286,  3047,\n",
      "          1366,    13,   198,   464,  3303,  2746,   318,   262,  5072,   286,\n",
      "           262,  4673,  1429,    13,   383,  3303,  2746,   318,  8776,   416,\n",
      "         13157,   355,   867,  8405,   355,  1744,   422,   262,  5981,  1366,\n",
      "           900,   290,  4673,   262,  5981, 10007,   290,    14,   273,   262,\n",
      "          5981, 19590,    13,   198,  1890,  1672]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 2. Generate\n",
    "peft_model.eval()  # make sure model is in evaluation mode\n",
    "with torch.no_grad():\n",
    "    out_tokens = peft_model.generate(\n",
    "        **model_input,       # unpack input_ids and attention_mask\n",
    "        max_new_tokens=100,  # How many new tokens to generate\n",
    "        do_sample=True,      # Greedy search or sampling?\n",
    "        top_p=0.95,          # Consider only minimal amount of tokens that cover 95% of overall probability\n",
    "        temperature=0.7,     # Skew the probability -> the lower the number more deterministic\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "print(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are large language models?\n",
      "Large language models (LLM) are language models that are extremely large in both the number of parameters and in the size of their training data. They are more difficult to train, but are capable of performing tasks that require large numbers of parameters or large amounts of training data.\n",
      "The language model is the output of the learning process. The language model is trained by collecting as many samples as possible from the relevant data set and learning the relevant parameters and/or the relevant weights.\n",
      "For example\n"
     ]
    }
   ],
   "source": [
    "# 3. Convert back to text\n",
    "print(tokenizer.decode(out_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put this into function\n",
    "def generate_response(input, max_new_tokens=100, do_sample=True, top_p=0.95, temperature=0.7):\n",
    "  batch = tokenizer(input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "  peft_model.eval()\n",
    "  with torch.no_grad():\n",
    "    out_tokens = peft_model.generate(\n",
    "        **batch,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "  model.train()\n",
    "  out = tokenizer.decode(out_tokens[0])\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In order to learn how to draw you need to learn the basics.\n",
      "This tutorial explains how to draw a dog.\n",
      "Step 1\n",
      "The first thing you need to do is draw a box. I will use the box for my dog’s head.\n",
      "Step 2\n",
      "First draw a rectangle of about 7.5 inches wide and 5.5 inches high.\n",
      "Step 3\n",
      "Now draw a circle about 1 inch in diameter. This will be the dog’s head.\n",
      "Step 4\n",
      "Draw a circle about 1.5\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(\"In order to learn how to draw you need to\")\n",
    "print(\"\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_response(\"Write 5 steps to learn how to draw\")\n",
    "print(\"\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To align our model to answer human questions and follow instructions we will fine-tune it on a dataset containing instructions and desired answers.\n",
    "We will use alpaca, version that has train-validation-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"disham993/alpaca-train-validation-test-split\")\n",
    "\n",
    "train = dataset['train']\n",
    "val = dataset['validation']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets usually contain instruction/input/question and output/answer etc. In the end model needs one field where everything is combined, like the \"text\" field here. This \"text\" field usually follows a specific format that we can create. After fine-tuning, during inference we will get the best results if we follow that format.\n",
    "\n",
    "We will use the format provided in the \"text\" field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format if input field is not empty:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "\n",
    "Format if input field is empty:\n",
    "```\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to fine-tune the model only on the responses, not the whole prompt text, we use a data collator that makes sure to calculate the loss only for the response. That is why we need standardized response format. We tell the collator after which phrase the response starts. Then it uses the tokenizer to convert that phrase into a list of tokens. When that list of tokens is found in the prompt, it knows that is should count the loss only for tokens that come after it. If it can't find that list of tokens in the prompt, it will raise a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM, SFTTrainer\n",
    "\n",
    "response_template = \"\\n### Response:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we use `SFTTrainer` from `trl` library (also in the hugging face ecosystem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results_2023_10\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    max_steps=1000,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    logging_first_step=True,\n",
    "    save_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradiens to have larger batch size while keeping GPU memory usage low\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0,\n",
    "    fp16=True, # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
    "    group_by_length = True,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val.select(indices=range(0, 500)),  # Let's take a subset of val so that evaluation is faster\n",
    "    dataset_text_field=\"text\",  # Field that contains the final prompt to use\n",
    "    data_collator=collator,     # Send our collator that will calculate loss only for response\n",
    "    args=training_arguments,\n",
    "    max_seq_length=384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(\"falcon-rw-1b-alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_efficient_llm_tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

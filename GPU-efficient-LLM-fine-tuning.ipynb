{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZONzSjCmQA7x"
      },
      "source": [
        "# GPU Efficient LLM fine-tuning\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github//velebit-ai/research-llm-development/blob/master/GPU-efficient-LLM-fine-tuning.ipynb)\n",
        "\n",
        "We'll go through an example of parameter efficinet GPU training on a T4 GPU by using Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0JdZQBAQA73"
      },
      "source": [
        "## Package setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-gmS0mg7QA74"
      },
      "outputs": [],
      "source": [
        "%pip install transformers -q\n",
        "%pip install bitsandbytes -q\n",
        "%pip install datasets -q\n",
        "%pip install accelerate -q\n",
        "%pip install peft -q\n",
        "%pip install trl -q\n",
        "%pip install einops -q\n",
        "%pip install tensorboard -q\n",
        "\n",
        "%pip install watermark -q # version checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J23pdn0nQA77"
      },
      "outputs": [],
      "source": [
        "%load_ext watermark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMOr273cQA78"
      },
      "source": [
        "## Loading the model\n",
        "\n",
        "We will load the model by using the `transformers` library from Hugging Face.\n",
        "\n",
        "In order to fit really large models into a single GPU, you can\n",
        "load the model in half precision. Most LLMs are even trained in half precision (float16, bfloat16) and there is almost no performace loss compared with full precision (float32) training.\n",
        "\n",
        "If that is not enough, you can quantize the weights of the model to 8bit or even 4bit. For that we need the `accelerate` and `bitsandbytes` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaMW9cTgQA79",
        "outputId": "602da880-82fc-4fb8-b999-09ac723d0485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: base (GB): 1.324785664\n",
            "Model: fp16 (GB): 0.662392832\n",
            "Model: 8bit (GB): 0.359354368\n",
            "Model: 4bit (GB): 0.207835136\n"
          ]
        }
      ],
      "source": [
        "# Illustrate memory usage\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\").to(\"cuda\")\n",
        "model_fp16 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=\"auto\").to(\"cuda\") # torch_dtype=torch.float16\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True) # 8bit and 4bit models are automatically loaded to GPU\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_4bit=True)\n",
        "\n",
        "def print_model_gpu_usage(model, name):\n",
        "  params_size = sum([param.nelement() * param.element_size()\n",
        "                    for param in model.parameters()])  # in bytes\n",
        "  print(f\"Model: {name} (GB): {params_size/1e9}\")\n",
        "\n",
        "print_model_gpu_usage(model, \"base\")\n",
        "print_model_gpu_usage(model_fp16, \"fp16\")\n",
        "print_model_gpu_usage(model_8bit, \"8bit\")\n",
        "print_model_gpu_usage(model_4bit, \"4bit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBAogYoFQA7-"
      },
      "source": [
        "We decreased model memory usage from 1.32GB to 0.2GB ~ 15% of original size. But loading in 8bit and 4bit comes with a bit of performance degradation. Keep in mind that we also need to save some GPU memory for training, so we don't want to take the whole memory just by loading the model.\n",
        "\n",
        "If we are interested in only running the inference on short sequences, we do not need that much extra memory. Memory requirements also increase if we will be processing very long sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UD10adalQA7_"
      },
      "outputs": [],
      "source": [
        "# clear memory\n",
        "import gc\n",
        "import torch\n",
        "del model\n",
        "del model_fp16\n",
        "del model_8bit\n",
        "del model_4bit\n",
        "gc.collect()\n",
        "with torch.no_grad():\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M5WyBMMQA8A"
      },
      "source": [
        "## Load the model for Fine-Tuning\n",
        "Now we will load the model that we will be working with in 8bit. If you are working locally, after downloading the model weights for the first time they will be cached, so following model loadings will be much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0YO_ete3QA8B"
      },
      "outputs": [],
      "source": [
        "# Falcon base model for tuning\n",
        "model_name = \"tiiuae/falcon-rw-1b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSGQ7m-2QA8B",
        "outputId": "c4cf8db3-98ac-4c16-e06c-9373e7045ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: base (GB): 1.41529088\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, torch_dtype=\"auto\", load_in_8bit=True)\n",
        "\n",
        "print_model_gpu_usage(model, \"base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJWsl4fzQA8C",
        "outputId": "8a764ce1-ef45-4da9-ac10-ce9faf55172d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.word_embeddings.weight                           torch.bfloat16     True    torch.Size([50304, 2048])\n",
            "transformer.h.0.self_attention.query_key_value.weight        torch.int8         False   torch.Size([6144, 2048])\n",
            "transformer.h.0.self_attention.query_key_value.bias          torch.bfloat16     False   torch.Size([6144])\n",
            "transformer.h.0.self_attention.dense.weight                  torch.int8         False   torch.Size([2048, 2048])\n",
            "transformer.h.0.self_attention.dense.bias                    torch.bfloat16     False   torch.Size([2048])\n",
            "transformer.h.0.mlp.dense_h_to_4h.weight                     torch.int8         False   torch.Size([8192, 2048])\n",
            "transformer.h.0.mlp.dense_h_to_4h.bias                       torch.bfloat16     False   torch.Size([8192])\n",
            "transformer.h.0.mlp.dense_4h_to_h.weight                     torch.int8         False   torch.Size([2048, 8192])\n",
            "transformer.h.0.mlp.dense_4h_to_h.bias                       torch.bfloat16     False   torch.Size([2048])\n",
            "transformer.h.0.input_layernorm.weight                       torch.bfloat16     True    torch.Size([2048])\n",
            "transformer.h.0.input_layernorm.bias                         torch.bfloat16     True    torch.Size([2048])\n",
            "transformer.h.0.post_attention_layernorm.weight              torch.bfloat16     True    torch.Size([2048])\n",
            "transformer.h.0.post_attention_layernorm.bias                torch.bfloat16     True    torch.Size([2048])\n",
            "transformer.h.1.self_attention.query_key_value.weight        torch.int8         False   torch.Size([6144, 2048])\n",
            "transformer.h.1.self_attention.query_key_value.bias          torch.bfloat16     False   torch.Size([6144])\n",
            "transformer.h.1.self_attention.dense.weight                  torch.int8         False   torch.Size([2048, 2048])\n"
          ]
        }
      ],
      "source": [
        "# Print only first 15 layers\n",
        "for i, (name, param) in enumerate(model.named_parameters()):\n",
        "    print(f\"{name:60s} {str(param.dtype):18s} {str(param.requires_grad):6s}  {param.shape}\")\n",
        "    if i == 15:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL73z_GdQA8C"
      },
      "source": [
        "Loading model in 8bit changed attention query, key, value and mlp weight layers in transformer block to int8 and locked them.\n",
        "Only embedding, layernorm and classification head layers (not visible here) are kept unlocked and loaded in bfloat16.\n",
        "\n",
        "They are loaded in bfloat16 because we set `torch_dtype=\"auto\"` and in the model configuration file bfloat16 is the type used. You can see the model config file in Hugging Face Hub or write `model.config`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CRgku3XQA8C"
      },
      "source": [
        "## PEFT - Parameter Efficient Fine Tuning\n",
        "LLMs are huge! We can't train the whole model for multiple reasons:\n",
        "\n",
        "*   We probably do not have enough data (overfit issues)\n",
        "*   Not enough hardware to run it in a reasonable amount of time\n",
        "*   Very expensive to rent GPUs\n",
        "\n",
        "Solution is to train only a subset of parameters using techniques like LoRA.\n",
        "\n",
        "The idea is to freeze all layers and inject additional layers that will be trained. These additional layers are two lower rank matrices which means that they have less parameters than the original weight matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iq8a46sNQA8C"
      },
      "outputs": [],
      "source": [
        "import peft\n",
        "\n",
        "lora_config = peft.LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    inference_mode=False,\n",
        "    r=16,           # Rank of update matrices, lower the rank lower number of parameters to train\n",
        "    lora_alpha=32,  # LoRA scaling factor\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",  # Should bias parameter be trained (of the original model); if True this will change the base model biases not just Lora AB matrices\n",
        "    target_modules=[\"query_key_value\"] # Print model parameters name to figure out which modules to target\n",
        ")\n",
        "\n",
        "peft_model = peft.get_peft_model(model, lora_config)  # this changes the model in-place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aEozRtPQA8D",
        "outputId": "1a59e3b7-4808-459a-c15b-fcfcd719802e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.word_embeddings.weight                                          torch.bfloat16     False   torch.Size([50304, 2048])\n",
            "transformer.h.0.self_attention.query_key_value.weight                       torch.int8         False   torch.Size([6144, 2048])\n",
            "transformer.h.0.self_attention.query_key_value.bias                         torch.bfloat16     False   torch.Size([6144])\n",
            "transformer.h.0.self_attention.query_key_value.lora_A.default.weight        torch.float32      True    torch.Size([16, 2048])\n",
            "transformer.h.0.self_attention.query_key_value.lora_B.default.weight        torch.float32      True    torch.Size([6144, 16])\n",
            "transformer.h.0.self_attention.dense.weight                                 torch.int8         False   torch.Size([2048, 2048])\n",
            "transformer.h.0.self_attention.dense.bias                                   torch.bfloat16     False   torch.Size([2048])\n",
            "transformer.h.0.mlp.dense_h_to_4h.weight                                    torch.int8         False   torch.Size([8192, 2048])\n",
            "transformer.h.0.mlp.dense_h_to_4h.bias                                      torch.bfloat16     False   torch.Size([8192])\n",
            "transformer.h.0.mlp.dense_4h_to_h.weight                                    torch.int8         False   torch.Size([2048, 8192])\n",
            "transformer.h.0.mlp.dense_4h_to_h.bias                                      torch.bfloat16     False   torch.Size([2048])\n",
            "transformer.h.0.input_layernorm.weight                                      torch.bfloat16     False   torch.Size([2048])\n",
            "transformer.h.0.input_layernorm.bias                                        torch.bfloat16     False   torch.Size([2048])\n",
            "transformer.h.0.post_attention_layernorm.weight                             torch.bfloat16     False   torch.Size([2048])\n",
            "transformer.h.0.post_attention_layernorm.bias                               torch.bfloat16     False   torch.Size([2048])\n",
            "transformer.h.1.self_attention.query_key_value.weight                       torch.int8         False   torch.Size([6144, 2048])\n"
          ]
        }
      ],
      "source": [
        "# Print only first 15 layers\n",
        "for i, (name, param) in enumerate(model.named_parameters()):  # deliberately using model instead of peft_model\n",
        "    print(f\"{name:75s} {str(param.dtype):18s} {str(param.requires_grad):6s}  {param.shape}\")\n",
        "    if i == 15:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucN9IicRQA8D"
      },
      "source": [
        "We can see how peft added `lora_A` and `lora_B` layers (in float32) and locked all other layers. When multiplied, matrix B*A correspond to `query_key_value.weight` dimension but overall has fewer parameters.\n",
        "\n",
        "Those are the only layers we will be training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgpg3_3TQA8D",
        "outputId": "aef1d3ef-7934-4aad-8530-ee3a6662a088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,145,728 || all params: 1,314,770,944 || trainable%: 0.2392605354077554\n"
          ]
        }
      ],
      "source": [
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXpifBC3QA8D"
      },
      "source": [
        "## Tokenizer\n",
        "Tokenizer converts text into small chunks - tokens. Each token is associated with an ID. This list of token IDs is the actual input to LLMs.\n",
        "\n",
        "```\n",
        "#       Text     --->           Tokens               --->    Input ids\n",
        "\"This is great!\" ---> [\"This\", \"_is\", \"_great\", \"!\"] ---> [52, 345, 124, 13]\n",
        "````\n",
        "\n",
        "Note: It is important to use the same tokenizer that was used during model pretraining. In practice, this means loading the tokenizer with the same name as the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ynkeSMK5QA8E"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CVAvGGyTQA8E"
      },
      "outputs": [],
      "source": [
        "# Base models are often trained without padding tokens\n",
        "assert tokenizer.pad_token_id is None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xq91gJTWQA8E"
      },
      "outputs": [],
      "source": [
        "# Often used in various tutorials as a quick PoC fix\n",
        "# We'll see that this approach is not optimal\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmdrcYWWQA8E",
        "outputId": "c989baf7-00e7-4e67-d8cf-90af1840c38e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [21968, 3303, 4981, 389], 'attention_mask': [1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "tokenized_text = tokenizer(\"Large language models are\")\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP0IiqR3QA8E"
      },
      "source": [
        "When calling the tokenizer directly, it uses the `__call__` method which returns `input_ids` and `attention_mask` directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL-KGcTBQA8G",
        "outputId": "049e3cc1-157e-49d6-bbc1-b1b633bb2580"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Large', 'Ġlanguage', 'Ġmodels', 'Ġare']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "tokenizer.tokenize(\"Large language models are\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_WAYiH8QA8G"
      },
      "source": [
        "If you want to see the tokens first, you can use `tokenizer.tokenize` method.\n",
        "\n",
        "Or you can convert ids to tokens using `tokenizer.convert_ids_to_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZJ7H0J2QA8H",
        "outputId": "a5d985c5-a889-4c69-da3a-005af197719a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Large', 'Ġlanguage', 'Ġmodels', 'Ġare']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_niH4qi6QA8H"
      },
      "source": [
        "`attention_mask` is useful for batch processing. Different texts, when tokenized have different number of tokens, but in order to create a batch we need to have the same number of tokens. We can add some token to the end of sequences to make them have the same number of toknes. This token is called padding token. But when we process the text we want to ignore these artificially added tokens so we use `attention_mask`. It tells the attention mechanism which tokens to include and which to ignore during its calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qN5A4l-QA8H",
        "outputId": "eef53d32-6c05-4272-877e-bb7a15e2ae36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[21968, 26992, 1018, 496, 4981, 389], [40, 716, 50256, 50256, 50256, 50256]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 0, 0, 0, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tokenizer([\"Large lanugage models are\", \"I am\"], padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LelFYf9FQA8H"
      },
      "source": [
        "First sentece has 6 tokens, while the second one has only 2 tokens. So we need to add 4 padding tokens (with the ID = 50256). In this case they are paded from the right. Attention mask for first sentence contains only 1s because all tokens are relevant, while in the second sentece, last 4 tokens are padded so attention mask is 0 for these tokens. This way attention mechanism will know to look only first 2 tokens as the relevant ones for this example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbmVR0NIQA8H"
      },
      "source": [
        "In order to decode input ids back to text, we can use the `tokenizer.decode` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "PhIWgUd7QA8I",
        "outputId": "a6267db3-c225-4ff7-a499-803e4581ff0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Large language models are'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "tokenizer.decode(tokenized_text['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwPUsUGUQA8I",
        "outputId": "e87fc7fb-7a18-4d13-e55d-97fafebdf9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding token:  <|endoftext|>\n",
            "Padding token id:  50256\n"
          ]
        }
      ],
      "source": [
        "print(\"Padding token: \", tokenizer.pad_token)\n",
        "print(\"Padding token id: \", tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0NRWulXQA8I"
      },
      "source": [
        "## Inference\n",
        "\n",
        "In order to run inference, we need to:\n",
        "1. Tokenize the text in order to get `input_ids` and `attention_mask`\n",
        "2. Pass `input_ids` and `attention_mask` to `model.generate` function\n",
        "3. Decode the output of the model back to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R7w8-waQA8J",
        "outputId": "bb26f6fd-7286-4c25-ab88-fe88061ff135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[2061,  389, 1588, 3303, 4981,   30]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "text = \"What are large language models?\"\n",
        "\n",
        "# 1. Tokenize\n",
        "model_input = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")  # return input_ids and attention_mask as pytorch tensors (\"pt\")\n",
        "print(model_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnMj00mvQA8J",
        "outputId": "906afbdb-9b8e-4795-db14-8cfbf33f07bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2061,   389,  1588,  3303,  4981,    30,   198, 21968,  3303,  4981,\n",
            "           357,  3069, 10128,     8,   389,  4981,   326,  2193,   284,  2380,\n",
            "          1588,  6867,   286,  1366,   287,   257,   835,   326,   318,  2653,\n",
            "         15208, 14998,   540,    13,  1119,   389,  6032,   973,   329,  8861,\n",
            "           884,   355,  4046,  9465,    11,  2939,  9465,    11,  3288,  3303,\n",
            "          7587,    11,   290,  4572, 11059,    13,   198,   818,   428,  1281,\n",
            "            11,   356,   447,   247,   297,  3002,   262,  1708, 10233,    25,\n",
            "           198,    12,  1867,   318,   257,  1588,  3303,  2746,    30,   198,\n",
            "            12,  1867,   318,   262,  3580,  1022,   257,  1588,  3303,  2746,\n",
            "           290,   257,  2769,  4673,  2746,    30,   198,    12,  1374,   284,\n",
            "          1382,   257,  1588,  3303,  2746,    30]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# 2. Generate\n",
        "peft_model.eval()  # make sure model is in evaluation mode\n",
        "with torch.no_grad():\n",
        "    out_tokens = peft_model.generate(\n",
        "        **model_input,       # unpack input_ids and attention_mask\n",
        "        max_new_tokens=100,  # How many new tokens to generate\n",
        "        do_sample=True,      # Greedy search or sampling?\n",
        "        top_p=0.95,          # Consider only minimal amount of tokens that cover 95% of overall probability\n",
        "        temperature=0.4,     # Skew the probability -> the lower the number more deterministic\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "print(out_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v8Gh58fQA8J",
        "outputId": "57fc7b3a-a8da-40e8-807b-9abc7f66c7e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are large language models?\n",
            "Large language models (LLMs) are models that learn to represent large amounts of data in a way that is computationally tractable. They are typically used for tasks such as speech recognition, image recognition, natural language processing, and machine translation.\n",
            "In this post, we’ll cover the following topics:\n",
            "- What is a large language model?\n",
            "- What is the difference between a large language model and a deep learning model?\n",
            "- How to build a large language model?\n"
          ]
        }
      ],
      "source": [
        "# 3. Convert back to text\n",
        "print(tokenizer.decode(out_tokens[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lsg10wC2QA8P"
      },
      "outputs": [],
      "source": [
        "# Let's put this into function\n",
        "def generate_response(input, max_new_tokens=100, do_sample=True, top_p=0.95, temperature=0.4):\n",
        "  batch = tokenizer(input, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "  peft_model.eval()\n",
        "  with torch.no_grad():\n",
        "    out_tokens = peft_model.generate(\n",
        "        **batch,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "  out = tokenizer.decode(out_tokens[0])\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L5RFkOAQA8Q",
        "outputId": "0b4e7203-d85a-47cd-ec5e-a349ec16585e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "In order to learn how to draw you need to be able to draw. The first step is to understand how to draw.\n",
            "The next step is to learn how to draw the human body.\n",
            "The next step is to learn how to draw the human face.\n",
            "The next step is to learn how to draw the human figure.\n",
            "The next step is to learn how to draw the human head.\n",
            "The next step is to learn how to draw the human arm.\n",
            "The next step is to learn how to draw the human leg.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = generate_response(\"In order to learn how to draw you need to\")\n",
        "print(\"\\n\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU8KiHx1QA8Q",
        "outputId": "06966691-d6df-46db-cd1a-f8530dd55cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Write 5 steps to learn how to draw a cartoon character.\n",
            "Step 1: Draw the basic shape of the character.\n",
            "Step 2: Draw the details of the character.\n",
            "Step 3: Draw the facial features of the character.\n",
            "Step 4: Draw the body of the character.\n",
            "Step 5: Draw the hair of the character.\n",
            "Step 6: Draw the clothing of the character.\n",
            "Step 7: Draw the background of the character.\n",
            "Step 8: Draw the environment of the character.\n",
            "Step 9: Draw the\n"
          ]
        }
      ],
      "source": [
        "response = generate_response(\"Write 5 steps to learn how to draw\")\n",
        "print(\"\\n\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aPZazNTQA8Q"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fszFZDJ5QA8R"
      },
      "source": [
        "To align our model to answer human questions and follow instructions we will fine-tune it on a dataset containing instructions and desired answers.\n",
        "We will use alpaca, version that has train-validation-test splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "k-wftNPWQA8R"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"disham993/alpaca-train-validation-test-split\")\n",
        "\n",
        "train = dataset['train']\n",
        "val = dataset['validation']\n",
        "test = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA2f8isiQA8R",
        "outputId": "0ddce81b-2d74-4d60-dc6a-0c237335167f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Generate a story about a girl who discovers a magical item.',\n",
              " 'input': '',\n",
              " 'output': 'Once upon a time, there was a young girl named Leila. She was always dreaming of a life of adventure and mystery. One day, while wandering in the woods near her house, she stumbled upon a mysterious item. It was a small box made of a strange material she had never seen before and it was glowing with a magical light. When she opened the box, she was amazed to find a beautiful golden necklace with a glowing stone. She put on the necklace and with a spark of light it vanished into thin air. Suddenly, Leila found herself taken to a far away land, full of mysterious creatures and magical wonders.',\n",
              " 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate a story about a girl who discovers a magical item.\\n### Response:\\nOnce upon a time, there was a young girl named Leila. She was always dreaming of a life of adventure and mystery. One day, while wandering in the woods near her house, she stumbled upon a mysterious item. It was a small box made of a strange material she had never seen before and it was glowing with a magical light. When she opened the box, she was amazed to find a beautiful golden necklace with a glowing stone. She put on the necklace and with a spark of light it vanished into thin air. Suddenly, Leila found herself taken to a far away land, full of mysterious creatures and magical wonders.'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqx8PU1-QA8R"
      },
      "source": [
        "Datasets usually contain instruction/input/question and output/answer etc. In the end model needs one field where everything is combined, like the \"text\" field here. This \"text\" field usually follows a specific format that we can create. After fine-tuning, during inference we will get the best results if we follow that format.\n",
        "\n",
        "We will use the format provided in the \"text\" field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtw1WtvcQA8R"
      },
      "source": [
        "Format if input field is not empty:\n",
        "\n",
        "```\n",
        "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "```\n",
        "\n",
        "Format if input field is empty:\n",
        "```\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLDMJBh8QA8S"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFSpXOqtQA8S"
      },
      "source": [
        "Because we want to fine-tune the model only on the responses, not the whole prompt text, we use a data collator that makes sure to calculate the loss only for the response. That is why we need standardized response format. We tell the collator after which phrase the response starts. Then it uses the tokenizer to convert that phrase into a list of tokens. When that list of tokens is found in the prompt, it knows that is should count the loss only for tokens that come after it. If it can't find that list of tokens in the prompt, it will raise a warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3atXCAHxQA8S"
      },
      "outputs": [],
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM, SFTTrainer\n",
        "\n",
        "response_template = \"\\n### Response:\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR9mZJdmQA8S"
      },
      "source": [
        "To train the model we use `SFTTrainer` from `trl` library (also in the hugging face ecosystem)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_BWJHAVmQA8T"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results_2023_10\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    max_steps=800,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    logging_first_step=True,\n",
        "    save_steps=100,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradiens to have larger batch size while keeping GPU memory usage low\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0,\n",
        "    fp16=True, # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
        "    group_by_length = True\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=val.select(indices=range(0, 500)),  # Let's take a subset of val so that evaluation is faster\n",
        "    dataset_text_field=\"text\",  # Field that contains the final prompt to use\n",
        "    data_collator=collator,     # Send our collator that will calculate loss only for response\n",
        "    args=training_arguments,\n",
        "    max_seq_length=384\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "IJilNuo3QA8T",
        "outputId": "2e16b234-f404-4c72-e538-df84486c74b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [800/800 30:53, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.484100</td>\n",
              "      <td>1.518072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.497000</td>\n",
              "      <td>1.489013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.461900</td>\n",
              "      <td>1.477147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.425400</td>\n",
              "      <td>1.469830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.421300</td>\n",
              "      <td>1.465316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.403500</td>\n",
              "      <td>1.460158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.400900</td>\n",
              "      <td>1.456942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.410200</td>\n",
              "      <td>1.456036</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=800, training_loss=1.4383162270486354, metrics={'train_runtime': 1856.9249, 'train_samples_per_second': 3.447, 'train_steps_per_second': 0.431, 'total_flos': 5346718428364800.0, 'train_loss': 1.4383162270486354, 'epoch': 0.18})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Xxk-ZH_mQA8U"
      },
      "outputs": [],
      "source": [
        "# Save trained model\n",
        "trainer.model.save_pretrained(\"falcon-rw-1b-alpaca\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INFERENCE_TEMPLATE = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n### Response:\"\n",
        "\n",
        "instruction = \"Write 5 steps to learn how to draw\"\n",
        "inference_input = INFERENCE_TEMPLATE.format(instruction=instruction)\n",
        "\n",
        "print(inference_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FWa5weBc2ue",
        "outputId": "9f59f411-5b75-436e-f03c-e86b4fb64bc4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Write 5 steps to learn how to draw\n",
            "### Response:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QObeeKNQA8U",
        "outputId": "ae270a7a-ef42-43f1-fc83-4dd8cf635525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Write 5 steps to learn how to draw\n",
            "### Response:\n",
            "1. Learn the basics of drawing.\n",
            "2. Practice drawing.\n",
            "3. Draw from observation.\n",
            "4. Learn to draw from other artists.\n",
            "5. Learn to draw from your imagination.\n",
            "\n",
            "How to Draw a Cat Step by Step\n",
            "\n",
            "1. Draw a cat's body.\n",
            "2. Draw the cat's face.\n",
            "3. Draw the cat's fur.\n",
            "4. Draw the cat's paws.\n",
            "5. Draw the cat's tail.\n",
            "\n",
            "How to Draw a Dog Step by Step\n",
            "\n",
            "1. Draw the dog's body.\n",
            "2. Draw the dog's face.\n",
            "3. Draw the dog's fur.\n",
            "4. Draw the dog's paws.\n",
            "5. Draw the\n"
          ]
        }
      ],
      "source": [
        "# Much better than before fine-tuning, but it still has a hard time stopping when done.\n",
        "response = generate_response(inference_input, max_new_tokens=150, do_sample=True)\n",
        "print(\"\\n\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can try different generation parameters\n",
        "response = generate_response(inference_input, max_new_tokens=150, do_sample=False)\n",
        "print(\"\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yag8c8O5ccy2",
        "outputId": "6cfc5cd4-235b-49d9-8e53-10617b7d5752"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Write 5 steps to learn how to draw\n",
            "### Response:\n",
            "1. Learn the basics of drawing. \n",
            "2. Practice drawing. \n",
            "3. Learn to use different tools and materials. \n",
            "4. Learn to use perspective. \n",
            "5. Learn to use shading and lighting. \n",
            "\n",
            "6. Learn to use perspective and shading to create realistic images. \n",
            "7. Learn to use perspective and shading to create realistic images. \n",
            "8. Learn to use perspective and shading to create realistic images. \n",
            "9. Learn to use perspective and shading to create realistic images. \n",
            "10. Learn to use perspective and shading to create realistic images. \n",
            "\n",
            "11. Learn to use perspective and shading to create realistic images. \n",
            "12. Learn to use perspective and shading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge LoRA weights"
      ],
      "metadata": {
        "id": "yJCWWG3mjWvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = peft.PeftModel.from_pretrained(base_model, \"falcon-rw-1b-alpaca\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "Wsk0YOlli-Kb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(input, max_new_tokens=100, do_sample=True, top_p=0.95, temperature=0.4):\n",
        "  batch = tokenizer(input, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    out_tokens = model.generate(\n",
        "        **batch,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "  out = tokenizer.decode(out_tokens[0])\n",
        "  return out"
      ],
      "metadata": {
        "id": "LYG1pjuLkR7U"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(inference_input, max_new_tokens=150, do_sample=True, temperature=0.4)\n",
        "print(\"\\n\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDJDzj8UkVAx",
        "outputId": "317c84a0-50bc-47f0-f157-0fec86b354c0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Write 5 steps to learn how to draw\n",
            "### Response:\n",
            "1. Learn the basics of drawing. \n",
            "2. Practice drawing skills. \n",
            "3. Learn to use different materials. \n",
            "4. Learn to use different techniques. \n",
            "5. Learn to use different styles. \n",
            "\n",
            "The steps above are a good place to start if you're looking to learn how to draw. They provide a solid foundation for learning the basics and can help you develop your skills over time. Practice drawing skills, learn to use different materials, learn to use different techniques, and learn to use different styles. These steps can help you develop your skills and learn how to draw.\n",
            "\n",
            "1. Learn the basics of drawing. \n",
            "2. Practice drawing skills. \n",
            "3. Learn to use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to Hub"
      ],
      "metadata": {
        "id": "cRvJSQvAq3nT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJSIBkilrPtV",
        "outputId": "f05f9c78-fdf5-4cc0-b7b0-1a78e5959219"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"falcon-rw-1b-alpaca\", use_temp_dir=False)\n",
        "tokenizer.push_to_hub(\"falcon-rw-1b-alpaca\", use_temp_dir=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88,
          "referenced_widgets": [
            "a62950a904bb4a768b29e016e944168c",
            "88c815323b634f4797006c4ac1b1052b",
            "d20fd3332a624a099f79fa8375da5cc6",
            "5f61ee0d6034444d92dcb9dbc264ba0e",
            "8d1d238c68334c5c8e921d0cdbd595f5",
            "c12d53c5f624425fb9d5a1aa59a41231",
            "6661cbde86444deaba22c4af1442a976",
            "81a2e347ee5c464aa6345c20221f3841",
            "13779332909a4a30be5c6f732ab3c4fc",
            "6b9cb9347d0d46beb7846571e50b9989",
            "41c745ca15204069b98f40c65485e779"
          ]
        },
        "id": "8UZZAF8Zq2HC",
        "outputId": "c51dd414-48c4-41f3-94ac-993e6e5c5a75"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a62950a904bb4a768b29e016e944168c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/mfernezir/falcon-rw-1b-alpaca/commit/133698c6ed946d755c23dfb482189acf18a9ff04', commit_message='Upload tokenizer', commit_description='', oid='133698c6ed946d755c23dfb482189acf18a9ff04', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework exercises\n",
        "\n",
        "- Try different inference and fine-tuning parameters\n",
        "- Fine-tune another model, such as LLaMA 2. Reference: https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html\n",
        "- Use separate EOS, BOS, and Padding tokens to improve the model\n",
        "\n",
        "Currently, by using EOS token as padding, the model never learns to predict the EOS token (model never predicts padding tokens since they are masked). The current model starts generating instructions and answering questions, but it stil doesn't know when to stop.\n"
      ],
      "metadata": {
        "id": "hlDJ2vPoteZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P7w8Zchvtnnk"
      },
      "execution_count": 39,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a62950a904bb4a768b29e016e944168c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88c815323b634f4797006c4ac1b1052b",
              "IPY_MODEL_d20fd3332a624a099f79fa8375da5cc6",
              "IPY_MODEL_5f61ee0d6034444d92dcb9dbc264ba0e"
            ],
            "layout": "IPY_MODEL_8d1d238c68334c5c8e921d0cdbd595f5"
          }
        },
        "88c815323b634f4797006c4ac1b1052b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c12d53c5f624425fb9d5a1aa59a41231",
            "placeholder": "​",
            "style": "IPY_MODEL_6661cbde86444deaba22c4af1442a976",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "d20fd3332a624a099f79fa8375da5cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81a2e347ee5c464aa6345c20221f3841",
            "max": 2623348889,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13779332909a4a30be5c6f732ab3c4fc",
            "value": 2623348889
          }
        },
        "5f61ee0d6034444d92dcb9dbc264ba0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b9cb9347d0d46beb7846571e50b9989",
            "placeholder": "​",
            "style": "IPY_MODEL_41c745ca15204069b98f40c65485e779",
            "value": " 2.62G/2.62G [01:07&lt;00:00, 44.4MB/s]"
          }
        },
        "8d1d238c68334c5c8e921d0cdbd595f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c12d53c5f624425fb9d5a1aa59a41231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6661cbde86444deaba22c4af1442a976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81a2e347ee5c464aa6345c20221f3841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13779332909a4a30be5c6f732ab3c4fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b9cb9347d0d46beb7846571e50b9989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c745ca15204069b98f40c65485e779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
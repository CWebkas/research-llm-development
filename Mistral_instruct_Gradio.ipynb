{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Mistral Chat API Demo\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/velebit-ai/research-llm-development/blob/master/Mistral_instruct_Gradio.ipynb)\n",
        "\n",
        "Mistral: state of the art free LLM (Septemeber 27, 2023)\n",
        "\n",
        "Learn more in the official Mistral [blog](https://mistral.ai/news/announcing-mistral-7b/)\n",
        "\n",
        "- Apache 2.0\n",
        "- Beats 13B LLaMA 2\n",
        "- No guardrails or moderation! (\"quick PoC instruction tuning\")\n",
        "- Base model and chat version\n",
        "\n",
        "Colab code adapted from the Hugging Face Space [demo](https://huggingface.co/spaces/osanseviero/mistral-super-fast/tree/main), commit `17fba4214d7237c5c1888bf2c24f3d8d657eced1`"
      ],
      "metadata": {
        "id": "zUpL2RQpClJ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EAQV-dpocJ7R"
      },
      "outputs": [],
      "source": [
        "!pip install gradio watermark -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark"
      ],
      "metadata": {
        "id": "MYCq_tNoD28d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "RtMeO2i_D962"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -p huggingface_hub,gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFxCtJNHEH2A",
        "outputId": "e2d15f6f-ea6c-4d7a-c7ce-6d588e8879d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface_hub: 0.17.3\n",
            "gradio         : 3.47.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT56FNOoFBpz",
        "outputId": "c6112ee4-0e98-43fa-cbd8-48b4c6616807"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last updated: 2023-10-09T15:39:54.353392+00:00\n",
            "\n",
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "Compiler    : GCC 11.4.0\n",
            "OS          : Linux\n",
            "Release     : 5.15.120+\n",
            "Machine     : x86_64\n",
            "Processor   : x86_64\n",
            "CPU cores   : 2\n",
            "Architecture: 64bit\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = InferenceClient(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        ")"
      ],
      "metadata": {
        "id": "51oWJA-4ciyd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(message, history):\n",
        "  prompt = \"<s>\"\n",
        "  for user_prompt, bot_response in history:\n",
        "    prompt += f\"[INST] {user_prompt} [/INST]\"\n",
        "    prompt += f\" {bot_response}</s> \"\n",
        "  prompt += f\"[INST] {message} [/INST]\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "SvSckEA8cekK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    prompt, history, temperature=0.9, max_new_tokens=256, top_p=0.95, repetition_penalty=1.0,\n",
        "):\n",
        "    temperature = float(temperature)\n",
        "    if temperature < 1e-2:\n",
        "        temperature = 1e-2\n",
        "    top_p = float(top_p)\n",
        "\n",
        "    generate_kwargs = dict(\n",
        "        temperature=temperature,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        do_sample=True,\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    formatted_prompt = format_prompt(prompt, history)\n",
        "\n",
        "    stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=True, details=True, return_full_text=False)\n",
        "    output = \"\"\n",
        "\n",
        "    for response in stream:\n",
        "        output += response.token.text\n",
        "        yield output\n",
        "    return output\n",
        "\n",
        "\n",
        "additional_inputs=[\n",
        "    gr.Slider(\n",
        "        label=\"Temperature\",\n",
        "        value=0.9,\n",
        "        minimum=0.0,\n",
        "        maximum=1.0,\n",
        "        step=0.05,\n",
        "        interactive=True,\n",
        "        info=\"Higher values produce more diverse outputs\",\n",
        "    ),\n",
        "    gr.Slider(\n",
        "        label=\"Max new tokens\",\n",
        "        value=256,\n",
        "        minimum=0,\n",
        "        maximum=8192,\n",
        "        step=64,\n",
        "        interactive=True,\n",
        "        info=\"The maximum numbers of new tokens\",\n",
        "    ),\n",
        "    gr.Slider(\n",
        "        label=\"Top-p (nucleus sampling)\",\n",
        "        value=0.90,\n",
        "        minimum=0.0,\n",
        "        maximum=1,\n",
        "        step=0.05,\n",
        "        interactive=True,\n",
        "        info=\"Higher values sample more low-probability tokens\",\n",
        "    ),\n",
        "    gr.Slider(\n",
        "        label=\"Repetition penalty\",\n",
        "        value=1.2,\n",
        "        minimum=1.0,\n",
        "        maximum=2.0,\n",
        "        step=0.05,\n",
        "        interactive=True,\n",
        "        info=\"Penalize repeated tokens\",\n",
        "    )\n",
        "]\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.ChatInterface(\n",
        "        generate,\n",
        "        additional_inputs=additional_inputs,\n",
        "    )\n",
        "\n",
        "demo.queue().launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "opVuYXWTcOSA",
        "outputId": "e9dcffeb-63bd-4902-c639-7c20cdaf0890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://16df1f0e05d55939b2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://16df1f0e05d55939b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OOE0hyZ9cWa3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}